{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/nnthao/miniconda3/envs/mat_inpainting/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import cv2\n",
    "from models.common import DetectMultiBackend\n",
    "# from utils.dataloaders import LoadImages\n",
    "from utils.general import Profile, check_img_size, non_max_suppression, scale_boxes\n",
    "from utils.torch_utils import select_device\n",
    "# from ultralytics.utils.plotting import Annotator\n",
    "\n",
    "# __file__ = '/home/nnthao/project/yolov5/detect.py'\n",
    "# FILE = Path(__file__).resolve()\n",
    "# ROOT = FILE.parents[0]  # YOLOv5 root directory\n",
    "# if str(ROOT) not in sys.path:\n",
    "#     sys.path.append(str(ROOT))  # add ROOT to PATH\n",
    "# ROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dmm_tensor(t, p=''):\n",
    "    if type(t) == list:\n",
    "        print(p + 'list', len(t))\n",
    "        for i in t:\n",
    "            dmm_tensor(i, p + '  ')\n",
    "    else:\n",
    "        print(p + 'tensor', t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 ðŸš€ v7.0-249-gf400bba Python-3.8.18 torch-1.8.0+cu111 CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 267 layers, 46189053 parameters, 0 gradients, 107.9 GFLOPs\n"
     ]
    }
   ],
   "source": [
    "weights = '/media/nnthao/yolov5/runs/train/exp-mask/weights/best.pt'    # model path or triton URL\n",
    "data = '/home/nnthao/project/yolov5/data/datamask.yaml' # dataset.yaml path\n",
    "device = 'cpu'  # cuda device, i.e. 0 or 0,1,2,3 or cpu\n",
    "half=False  # use FP16 half-precision inference\n",
    "dnn=False   # use OpenCV DNN for ONNX inference\n",
    "imgsz = (512, 512)  # inference size (height, width)\n",
    "\n",
    "# Load model\n",
    "device = select_device(device)\n",
    "model = DetectMultiBackend(weights, device=device, dnn=dnn, data=data)\n",
    "stride, names, pt = model.stride, model.names, model.pt\n",
    "imgsz = check_img_size(imgsz, s=stride)  # check image size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source = '/home/nnthao/project/FDA/test_sets/CelebA-HQ/masked_images/test1.png' # file/dir/URL/glob/screen/0(webcam)\n",
    "# vid_stride=1    # video frame-rate stride\n",
    "\n",
    "# # Dataloader\n",
    "# batch_size = 1\n",
    "# dataset = LoadImages(source, img_size=imgsz, stride=stride, auto=pt, vid_stride=vid_stride)\n",
    "# vid_path, vid_writer = [None] * batch_size, [None] * batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "infer time:  10.72494649887085\n",
      "nms time:  0.004666566848754883\n",
      "tensor transform time:  0.18992400169372559\n"
     ]
    }
   ],
   "source": [
    "augment=False   # augmented inference\n",
    "# save_dir = 'runs/detect/exp'\n",
    "visualize=False # visualize features\n",
    "conf_thres=0.25 # confidence threshold\n",
    "iou_thres=0.45  # NMS IOU threshold\n",
    "classes=None    # filter by class: --class 0, or --class 0 2 3\n",
    "agnostic_nms=False  # class-agnostic NMS\n",
    "max_det=18  # maximum detections per image\n",
    "\n",
    "batch_size = 1  # batch_size\n",
    "\n",
    "# line_thickness=1  # bounding box thickness (pixels)\n",
    "box_color = (255, 255, 255)\n",
    "\n",
    "# Run inference\n",
    "model.warmup(imgsz=(1 if pt or model.triton else batch_size, 3, *imgsz))  # warmup\n",
    "dt = (Profile(), Profile(), Profile())\n",
    "\n",
    "from torchvision.io import read_image\n",
    "images_in = ((torch.cat((read_image('/home/nnthao/project/FDA/test_sets/CelebA-HQ/images/test1.png')[None, ...], \n",
    "       read_image('/home/nnthao/project/FDA/test_sets/CelebA-HQ/images/test2.png')[None, ...]))) / 255).to(device)\n",
    "\n",
    "# for im in images_in:\n",
    "# for path, im, im0s, vid_cap, s in dataset:\n",
    "with dt[0]:\n",
    "    # images_in = torch.from_numpy(images_in).to(model.device)\n",
    "    images_in = images_in.half() if model.fp16 else images_in.float()  # uint8 to fp16/32\n",
    "    # images_in /= 255  # 0 - 255 to 0.0 - 1.0\n",
    "    if len(images_in.shape) == 3:\n",
    "        images_in = images_in[None]  # expand for batch dim\n",
    "\n",
    "# Inference\n",
    "with dt[1]:\n",
    "    # visualize = increment_path(save_dir / Path(path).stem, mkdir=True) if visualize else False\n",
    "    infer_st = time.time()\n",
    "    pred = model(images_in, augment=augment, visualize=visualize)\n",
    "    infer_t = time.time() - infer_st\n",
    "\n",
    "    print('infer time: ', infer_t)\n",
    "\n",
    "# NMS\n",
    "with dt[2]:\n",
    "    nms_st = time.time()\n",
    "    pred = non_max_suppression(pred, conf_thres, iou_thres, classes, agnostic_nms, max_det=max_det)\n",
    "    nms_t = time.time() - nms_st\n",
    "\n",
    "    print('nms time: ', nms_t)\n",
    "\n",
    "tensor_trans_st = time.time()\n",
    "\n",
    "# >>> use sefl-code >>>\n",
    "# im0 = torch.zeros_like(im[0][0])\n",
    "\n",
    "# for coor in pred[0][..., :-2].floor().int():\n",
    "#     for y in range(512):\n",
    "#         for x in range(512):\n",
    "#             if x < coor[2] and x > coor[0] and y < coor[3] and y > coor[1]:\n",
    "#                 im0[y][x] = 1\n",
    "# <<< use sefl-code <<<\n",
    "\n",
    "# >>> use Annotator >>>\n",
    "# for i, det in enumerate(pred):  # per image\n",
    "#     im0 = (im.permute(2, 3, 1, 0).reshape(512, 512, 3) * 255).floor().contiguous().cpu().detach().numpy()\n",
    "#     annotator = Annotator(im0, line_width=line_thickness)\n",
    "\n",
    "#     if len(det):\n",
    "#         # Rescale boxes from img_size to im0 size\n",
    "#         det[:, :4] = scale_boxes(im.shape[2:], det[:, :4], im0.shape).round()\n",
    "\n",
    "#         # Write results\n",
    "#         for *xyxy, conf, cls in reversed(det):\n",
    "#             annotator.box_label(xyxy, None, color=box_color)\n",
    "\n",
    "#     im0 = annotator.result()\n",
    "# <<< use Annotator <<<\n",
    "\n",
    "# >>> use cv2 >>>\n",
    "dets_in = None\n",
    "for i, det in enumerate(pred):  # per image\n",
    "    det_in = torch.zeros(512, 512, 3, dtype=torch.uint8).contiguous().cpu().detach().numpy()\n",
    "\n",
    "    if len(det):\n",
    "        # Rescale boxes from img_size to det_in size\n",
    "        det[:, :4] = scale_boxes(images_in.shape[2:], det[:, :4], det_in.shape).round()\n",
    "\n",
    "        # Write results\n",
    "        for *xyxy, conf, cls in reversed(det):\n",
    "            cv2.rectangle(det_in, (int(xyxy[0]), int(xyxy[1])), (int(xyxy[2]), int(xyxy[3])), box_color, -1)\n",
    "\n",
    "    det_in = torch.from_numpy(det_in[..., 0].reshape(1, 1, 512, 512)).to(model.device)\n",
    "    det_in = det_in.half() if model.fp16 else det_in.float()  # uint8 to fp16/32\n",
    "    det_in /= 255  # 0 - 255 to 0.0 - 1.0\n",
    "\n",
    "    if i:\n",
    "        dets_in = torch.cat((dets_in, det_in))\n",
    "    else:\n",
    "        dets_in = det_in\n",
    "# <<< use cv2 <<<\n",
    "\n",
    "tensor_trans_t = time.time() - tensor_trans_st\n",
    "print('tensor transform time: ', tensor_trans_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512, 512])\n",
      "torch.Size([1, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "for det_in in dets_in:\n",
    "    print(det_in.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "\n",
    "for det_in in dets_in:\n",
    "    mpimg.imsave('./imshow-ngu-vc.png', np.concatenate(((det_in.permute(1, 2, 0) * 255).floor().contiguous().cpu().detach().numpy(), \n",
    "                                                (det_in.permute(1, 2, 0) * 255).floor().contiguous().cpu().detach().numpy(), \n",
    "                                                (det_in.permute(1, 2, 0) * 255).floor().contiguous().cpu().detach().numpy()), axis=2).astype('uint8'))\n",
    "    input()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mat_inpainting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
